# ETL Pipelines with Machine Learning & MongoDB

## 📖 Project Description
This project implements **ETL pipelines** to extract, transform, and load (ETL) data from multiple sources, followed by applying machine learning models.  
The primary goal is to process diverse datasets related to **COVID-19** and store the cleaned and enriched data into **MongoDB** for further analysis.  

This project was completed as part of a university assignment, in collaboration with one of my colleagues.  

---

## ⚙️ Steps

### 🔹 Extract
- Data extracted from:  
  - CSV files  
  - A website  
  - An Arabic video about COVID-19  

### 🔹 Transform
- Cleaning and preprocessing raw data  
- Feature engineering and preparation  
- Applying **Machine Learning models** to enhance insights  

### 🔹 Load
- Storing all processed data into **MongoDB** for structured storage and querying  

---

## 🛠️ Technologies Used
- **Python** (for ETL and ML scripts)  
- **Pandas & NumPy** (data preprocessing)  
- **Sklearn** (machine learning)  
- **MongoDB** (data storage)  

---

## 🎯 Goal of the Project
The aim of this project is to:  
- Create a robust ETL pipeline for heterogeneous data sources.  
- Apply machine learning to transformed datasets for better insights.  
- Centralize all processed data into MongoDB for scalable storage and retrieval.  
